{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63cb574e-40fe-478c-94ad-8654c6c6a575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727e6fc1f0c548918fc598a3ffc36969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-14 09:30:17,688] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[INFO] [1702546220.403876]: VLM and vision service initialized\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f96f7c225c4d0a81b400b848ba68cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/4.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6262e677ef514a1fbeca8fe523e5b099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b406f0f699674288b97d98bfe622104c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)rocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e60e0a5fe04dd283a4ebed8c698a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from libzim.reader import Archive\n",
    "from libzim.search import Query, Searcher\n",
    "from libzim.suggestion import SuggestionSearcher\n",
    "\n",
    "from pipeline.prompt_templates import generic_prompt\n",
    "from llm_and_vision_node import Ros_llm_vision_talker\n",
    "\n",
    "from llava_utils import instantiate_llava, LangChainLLavaWrapper\n",
    "\n",
    "default_llm_device = 'cuda:1'\n",
    "\n",
    "# ROS linker to camera\n",
    "T = Ros_llm_vision_talker(camera_input_topic = \"/realsense/color/image_raw\",\n",
    "                         spin=False)\n",
    "T.load_image('img_db/0.jpg') # load img in case camera's not working\n",
    "\n",
    "# Vision-Language model (obviously including LLM).\n",
    "tokenizer, model, image_processor, context_len = instantiate_llava(device = default_llm_device)\n",
    "\n",
    "image_embedding_model = model.get_vision_tower() # CLIP encoder from LLaVa (frozen, not re-trained).\n",
    "\n",
    "prompt_tmpl = generic_prompt(chat_history_field = True)\n",
    "\n",
    "llm = LangChainLLavaWrapper(tokenizer = tokenizer, model = model, image_processor = image_processor, image_input = T, use_image_input=1,\n",
    "                           device = default_llm_device)\n",
    "llm.default_prompt = prompt_tmpl # for VQA as a LLM tool, the LLM will only input the text request(i.e. 'caption img') so we need a default prompt template.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f8f1e0-4566-4d96-b08b-902723900370",
   "metadata": {},
   "source": [
    "# For each image in dir, Generate partially unfilled JSON training data. (User can fill questions and answers manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ad28a0c-01f0-4c88-a881-33a7142386bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import uuid\n",
    "import typing\n",
    "from typing import List\n",
    "# Add this prefix so JSONs can be distinguished from labelme segmentation label JSONs.\n",
    "json_prefix = 'q_' # JSON name = json_prefix + img_name + .json.\n",
    "\n",
    "folder = 'img_db/testboard/'\n",
    "\n",
    "def generate_image_json_training_data(folder: str,\n",
    "                                      json_prefix: str = \"q_\",\n",
    "                                      allowed_img_formats: List[str] = [\".jpg\"],\n",
    "                                      human_question_templates: List[str] = [\"<image>\\n\",\n",
    "                                                                             \"How to disassemble?\"]):\n",
    "    \"\"\"Generate JSON files for image-based conversations.\n",
    "\n",
    "    Keyword arguments:\n",
    "    folder -- the folder where images are located and where JSON files will be created\n",
    "    json_prefix -- the string prefix that will be added to JSON file name. (to prevent collisions with labelme segmentation annotations, which are i.e. 0.json\"\n",
    "    allowed_img_formats -- JSONs will only be created for files (images) that are in one of these formats\n",
    "    #human_question_templates -- each JSON can contain several distinct conversations. For each template in human_question_templates, a conversation dict will\n",
    "    #be created, where the (human question) will be one of the elements in human_question_templates.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Each template must contain the image token!\n",
    "    image_token = \"<image>\\n\"\n",
    "    for i, tmp in enumerate(human_question_templates):\n",
    "        if image_token not in tmp:\n",
    "            human_question_templates[i] = image_token + tmp\n",
    "\n",
    "    json_template = {\"id\": \"unique_id\",\n",
    "                \"image\": 'img_filepath',\n",
    "                \"conversations\":[\n",
    "                          {\n",
    "                            \"from\": \"human\",\n",
    "                            \"value\": \"<image>\\nDescribe the image.\"\n",
    "                          },\n",
    "                          {\n",
    "                            \"from\": \"gpt\",\n",
    "                            \"value\": \"\"\n",
    "                          },]\n",
    "                }\n",
    "    conv_human_question_template = {\n",
    "                            \"from\": \"human\",\n",
    "                            \"value\": \"<image>\\nDescribe the image.\"\n",
    "    }\n",
    "    conv_ai_answer_template = {\n",
    "                            \"from\": \"gpt\",\n",
    "                            \"value\": \"\"\n",
    "                          }\n",
    "\n",
    "    # Keep only img files with allowed_img_formats\n",
    "    all_files = os.listdir(folder)\n",
    "    good_files = []\n",
    "    for file in all_files:\n",
    "        for format in allowed_img_formats:\n",
    "            if format in file:\n",
    "                good_files.append([file, file.split(format)[0]]) # A list of [[\"img.jpg\", \"img\"]\n",
    "                \n",
    "    # Make and save jsons for all files\n",
    "    for file_and_format, file in good_files:\n",
    "        template = copy.deepcopy(json_template)\n",
    "        template['id'] = uuid.uuid4().__str__() # bing, calculate probability of collision :D \n",
    "        template['image'] = folder+'/'+file_and_format\n",
    "        json_name = json_prefix + file + '.json'\n",
    "        full_json_save_path = folder + '/' + json_name\n",
    "\n",
    "        # Create conversation list of dicts, for each template in human_question_templates\n",
    "        conversations = []\n",
    "        #tmp_human_template = copy.deepcopy(conv_human_question_template)\n",
    "        for template_question in human_question_templates:\n",
    "            conv_human_question_template[\"value\"] = template_question\n",
    "            #print(template_question)\n",
    "            conversations.append(copy.deepcopy(conv_human_question_template))\n",
    "\n",
    "            # TODO use VLM to provide \"initial guess\" answer\n",
    "            conversations.append(conv_ai_answer_template)\n",
    "            \n",
    "        #print(conversations)\n",
    "        template['conversations'] = conversations\n",
    "        #print(full_json_save_path)\n",
    "        #json_string = json.dumps(obj = [template], indent = 2)\n",
    "        #print(json_string)\n",
    "        with open(full_json_save_path, 'w') as f:\n",
    "            json.dump([template], f)\n",
    "        #print(template)\n",
    "    #print(template)\n",
    "\n",
    "generate_image_json_training_data(folder = 'img_db/testboard')\n",
    "generate_image_json_training_data(folder = 'img_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6d46f4-62ac-49af-8cf3-2a9e5bf8921a",
   "metadata": {},
   "source": [
    "# Use LLM to generate \"human\" questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb82ef0-28f7-410e-8468-574166183658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e16a2bb-7142-4c2a-ad23-e06bd5c2a96d",
   "metadata": {},
   "source": [
    "# Testing opening ZIM archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c8000-406d-4e30-9eb9-7bd675c88503",
   "metadata": {},
   "outputs": [],
   "source": [
    "zim = Archive(\"test.zim\")\n",
    "print(f\"Main entry is at {zim.main_entry.get_item().path}\")\n",
    "entry = zim.get_entry_by_path(\"home/fr\")\n",
    "print(f\"Entry {entry.title} at {entry.path} is {entry.get_item().size}b.\")\n",
    "print(bytes(entry.get_item().content).decode(\"UTF-8\"))\n",
    "\n",
    "# searching using full-text index\n",
    "search_string = \"Welcome\"\n",
    "query = Query().set_query(search_string)\n",
    "searcher = Searcher(zim)\n",
    "search = searcher.search(query)\n",
    "search_count = search.getEstimatedMatches()\n",
    "print(f\"there are {search_count} matches for {search_string}\")\n",
    "print(list(search.getResults(0, search_count)))\n",
    "\n",
    "# accessing suggestions\n",
    "search_string = \"kiwix\"\n",
    "suggestion_searcher = SuggestionSearcher(zim)\n",
    "suggestion = suggestion_searcher.suggest(search_string)\n",
    "suggestion_count = suggestion.getEstimatedMatches()\n",
    "print(f\"there are {suggestion_count} matches for {search_string}\")\n",
    "print(list(suggestion.getResults(0, suggestion_count)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
